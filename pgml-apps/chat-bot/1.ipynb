{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Chatbot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the necessary libraries and load env variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pgml\n",
    "import pandas as pd\n",
    "from pgml import Database\n",
    "from psycopg_pool import ConnectionPool\n",
    "import openai\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_KEY\")\n",
    "conn_str = os.getenv(\"PGML_CONNECTION_STR\")\n",
    "collection_name = os.getenv(\"PGML_COLLECTION_NAME\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the data\n",
    "\n",
    "Here we retrieve the contents of the markdown files from the postgresml website and prepare the data to be upserted into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_suite = pd.read_csv('data/test-suite.csv')\n",
    "\n",
    "## recursively go through '../../pgml-dashboard/content' and save all markdown files content to df\n",
    "def read_markdown_files(directory):\n",
    "    data = []\n",
    "    base_url = \"http://postgresml.com/\"\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for file in glob.glob(os.path.join(dirpath, '*.md')):\n",
    "            with codecs.open(file, 'r', 'utf-8') as f:\n",
    "                # Compute the relative file path\n",
    "                relative_path = os.path.relpath(file, directory)\n",
    "                # Remove '.md' from the end\n",
    "                relative_path = relative_path[:-3]\n",
    "                # Prepend the base URL\n",
    "                url = base_url + relative_path\n",
    "                data.append((f.read(), url))\n",
    "\n",
    "    # Convert the list of tuples into a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['text', 'url'])\n",
    "\n",
    "    return df.to_dict('records')\n",
    "documents = read_markdown_files('../../pgml-dashboard/content')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to pgml collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Database(conn_str)\n",
    "collection = await db.create_or_get_collection(collection_name)\n",
    "await collection.upsert_documents(documents)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create utility functions\n",
    "\n",
    "Created some utility functions that are used in the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# import random\n",
    "import openai\n",
    "\n",
    "def get_similarity(string1, string2):\n",
    "    pool = ConnectionPool(conn_str)\n",
    "    # Return a random floating point number between 0 and 1\n",
    "    # return random.uniform(0, 1)\n",
    "    # Establish a database connection\n",
    "    conn = pool.getconn()\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        # Execute a SQL query that computes the cosine similarity between two text strings\n",
    "        cur.execute(\"\"\"\n",
    "        WITH string1 AS (\n",
    "            SELECT pgml.embed(\n",
    "            transformer => 'hkunlp/instructor-xl', \n",
    "            text => %s,\n",
    "            kwargs => '{\"instruction\": \"Represent the answer: \"}'\n",
    "            ) AS embedding\n",
    "            ),\n",
    "            string2 AS (\n",
    "                SELECT pgml.embed(\n",
    "                transformer => 'hkunlp/instructor-xl', \n",
    "                text => %s,\n",
    "                kwargs => '{\"instruction\": \"Represent the answer: \"}'\n",
    "                ) AS embedding\n",
    "            )\n",
    "            SELECT pgml.cosine_similarity(string1.embedding, string2.embedding) AS similarity\n",
    "            FROM string1, string2;\n",
    "        \"\"\", (string1, string2))\n",
    "        # Fetch the computed similarity\n",
    "        similarity = cur.fetchone()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        # Close the cursor\n",
    "        cur.close()\n",
    "        return similarity\n",
    "\n",
    "def get_completion(prompt, model_name):\n",
    "    # Create a chat completion using the OpenAI API\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=model_name,\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},    \n",
    "      ]\n",
    "    )\n",
    "\n",
    "    return completion\n",
    "\n",
    "def get_prediction(context, question, model_name):\n",
    "    # Format the prompt to be sent to the model\n",
    "    prompt = f\"\"\"You are a helpful assistant that answers users questions about PostgresML. Use the context below, which is delimited by three backticks to help answer the users question.\n",
    "    context: ```{context}```\n",
    "\n",
    "    user: {question}\n",
    "\n",
    "    assistant:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a chat completion using the OpenAI API\n",
    "    completion = get_completion(prompt, model_name)\n",
    "\n",
    "    # Extract the model's answer from the API response\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def check_accuracy(question, correct_answer, prediction):\n",
    "    # Format the prompt to be sent to the model\n",
    "    prompt = f\"\"\"You are an expert in determining if the answer to a question is correct. Below is the 'question', 'correct answer', and 'possible answer'. Each is delimited by three backticks. Your job is to determine if the 'possible answer' is correct. You compare it to the 'correct answer' to determine this.\n",
    "\n",
    "Return your answer as JSON with the key 'accurate' which is a boolean of true of false.\n",
    "\n",
    "question: ```{question}```\n",
    "\n",
    "correct answer: ```{correct_answer}```\n",
    "\n",
    "possible answer: ```{prediction}```\n",
    "\n",
    "Don't explain your reasoning, just return the json.\n",
    "lets think through this step by step and determine if possible answer is correct.\"\"\"\n",
    "\n",
    "    # Create a chat completion using the OpenAI API\n",
    "    completion = get_completion(prompt, \"gpt-4\")\n",
    "\n",
    "    # Extract the model's answer from the API response and parse the JSON string\n",
    "    answer = json.loads(completion.choices[0].message.content)\n",
    "\n",
    "    # Check if 'accurate' is a boolean, if not, raise an error\n",
    "    if type(answer['accurate']) != bool:\n",
    "        raise ValueError(\"answer.accurate must be a boolean\")\n",
    "\n",
    "    # If the prediction is accurate, return 1, otherwise return 0\n",
    "    return 1 if answer['accurate'] else 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitters = [\n",
    "    {\n",
    "        \"splitter_name\": \"recursive_character\",\n",
    "        \"splitter_params\": {\"chunk_size\": 1500, \"chunk_overlap\": 40},\n",
    "        \"name\": 5,\n",
    "    },\n",
    "    {\n",
    "        \"splitter_name\": \"recursive_character\",\n",
    "        \"splitter_params\": {\"chunk_size\": 2000, \"chunk_overlap\": 40},\n",
    "        \"name\": 6,\n",
    "    },\n",
    "    {\n",
    "        \"splitter_name\": \"recursive_character\",\n",
    "        \"splitter_params\": {\"chunk_size\": 2500, \"chunk_overlap\": 40},\n",
    "        \"name\": 7,\n",
    "    },    \n",
    "]\n",
    "\n",
    "embedding_models = [  \n",
    "    {'model_name': \"hkunlp/instructor-xl\", 'model_params': {\"instruction\": \"Represent the document for retrieval: \"}, 'query_params': {\"instruction\": \"Represent the question for retrieving supporting documents: \"}, \"name\": 3},\n",
    "    {'model_name': \"intfloat/e5-small\", 'model_params': {}, 'query_params': {}, \"name\": 1},\n",
    "]\n",
    "\n",
    "completion_models = [\n",
    "    {'model_name': \"gpt-3.5-turbo\", \"name\": 1},\n",
    "    {'model_name': \"gpt-4\", \"name\": 2},\n",
    "]\n",
    "\n",
    "grid = {\n",
    "    \"splitters\": splitters,\n",
    "    \"models\": embedding_models,\n",
    "    \"completion_models\": completion_models,\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_search_configurations(collection, grid):\n",
    "    # Generate the configurations for the search operation\n",
    "    configurations = []\n",
    "    for splitter, model, completion_model in itertools.product(grid[\"splitters\"], grid[\"models\"], grid[\"completion_models\"]):\n",
    "        embedding_model_id = await collection.register_model(\n",
    "            model_name=model[\"model_name\"],\n",
    "            model_params= model[\"model_params\"],            \n",
    "        )\n",
    "        splitter_id = await collection.register_text_splitter(\n",
    "            splitter_name=splitter[\"splitter_name\"],\n",
    "            splitter_params=splitter[\"splitter_params\"],        \n",
    "        )\n",
    "        ## append id to model and splitter \n",
    "        model['id'] = embedding_model_id\n",
    "        splitter['id'] = splitter_id        \n",
    "        await collection.generate_chunks(splitter_id=splitter_id)               \n",
    "        await collection.generate_embeddings(model_id=embedding_model_id, splitter_id=splitter_id)        \n",
    "        configurations.append((splitter, model, completion_model))\n",
    "    return configurations\n",
    "\n",
    "\n",
    "async def generate_predictions(collection, test_suite_copy, configuration):\n",
    "    # Generate the predictions for the test suite copy based on the configuration\n",
    "    splitter, model, completion_model = configuration\n",
    "    for index, row in test_suite_copy.iterrows():        \n",
    "        res = await collection.vector_search(\n",
    "            row['input'],\n",
    "            top_k=2,\n",
    "            model_id=model['id'],\n",
    "            splitter_id=splitter[\"id\"],\n",
    "            query_params=model.get('query_params', {})\n",
    "        )        \n",
    "\n",
    "        # Generate prediction by accumulating chunks and feeding to completion model\n",
    "        chunks = [r[1] for r in res]\n",
    "        completion = get_prediction(chunks, row['input'], completion_model['model_name'])\n",
    "        test_suite_copy.at[index, 'prediction'] = completion\n",
    "\n",
    "        # Calculate and store similarity measure\n",
    "        test_suite_copy.at[index, 'prediction_similarity'] = get_similarity(row['output'], completion)\n",
    "\n",
    "        # Check and store accuracy of prediction\n",
    "        test_suite_copy.at[index, 'accuracy'] = check_accuracy(row['input'], row['output'], completion)\n",
    "\n",
    "    # Set ids for the different models used in the test\n",
    "    test_suite_copy['embedding_model_id'] = model['name']\n",
    "    test_suite_copy['splitter_id'] = splitter[\"name\"]\n",
    "    test_suite_copy['completion_model_id'] = completion_model[\"name\"]\n",
    "\n",
    "    return test_suite_copy\n",
    "\n",
    "\n",
    "async def grid_search(test_suite, grid):    \n",
    "\n",
    "    # Initializing connection to database\n",
    "    db = Database(conn_str)\n",
    "\n",
    "    # Create or get the collection from database\n",
    "    collection = await db.create_or_get_collection(collection_name)\n",
    "\n",
    "    # Generate the configurations to test\n",
    "    configurations = await generate_search_configurations(collection, grid)\n",
    "\n",
    "    # Generate predictions for each configuration and store the results\n",
    "    results = []\n",
    "    for configuration in configurations:\n",
    "        test_suite_copy = test_suite.copy()\n",
    "        prediction = await generate_predictions(collection, test_suite_copy, configuration)\n",
    "        results.append(prediction)\n",
    "\n",
    "    return results\n",
    "\n",
    "grid_search_res_list = await grid_search(test_suite, grid)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(grid_search_res_list):\n",
    "    # Concatenate list, group by ids, get mean, reset index and sort all in one line\n",
    "    res_df = pd.concat(grid_search_res_list).groupby(['embedding_model_id', 'splitter_id', 'completion_model_id']).mean().reset_index().sort_values(by=['accuracy'], ascending=False)\n",
    "    \n",
    "    # Add a new column called model_id and set it to the index\n",
    "    res_df['model_id'] = res_df.index.astype(str)\n",
    "\n",
    "    return res_df\n",
    "\n",
    "results = format_results(grid_search_res_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaulate results and write results to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def generate_report_and_save(results, model=\"gpt-4\"):\n",
    "    results_list = results.to_dict('records')\n",
    "    # Prepare the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert mathmetician and data scientist and machine learning You are tasked with evaluating which model_id has the highest accuracy. The results of the evaluation is below, delimited by three backticks. The results is a list of results, where each item is a model. 'completion_model_id', 'splitter_id', and 'embedding_model_id; are the features in the model. 'prediction_similarity' is the mean of the prediction in the prediction. 'accuracy' is the mean of the accuracy of the prediction. 'accuracy' is the metric you are trying to maximize. prediction_similarity is the similarity of the prediction to the correct answer. One would assume that the higher the prediction_similarity, the higher the accuracy is, but that may not always be the case. Analyze the results and determine which model is the best.\n",
    "\n",
    "    results: ```{results_list}```\n",
    "\n",
    "    return your answer as JSON with the keys \"winning_model_id\" and \"report\" \n",
    "\n",
    "    \"winning_model_id\" is the model_id of the most accurate model. \"report\" is a string that explains how you came to your conclusion.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Create completion\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": 'assistant', \"content\": \"\"\"{\"winning_model_id\": \"model_id\", \"report\": \"After analyzing the results I have determined that\"}\"\"\"},            \n",
    "            {\"role\": 'assistant', \"content\": \"\"\"{\"winning_model_id\": \"model_id\", \"report\": \"After analyzing the results I have determined that\"}\"\"\"},            \n",
    "            {\"role\": 'assistant', \"content\": \"\"\"{\"winning_model_id\": \"model_id\", \"report\": \"After analyzing the results I have determined that\"}\"\"\"},            \n",
    "            {\"role\": \"system\", \"content\": prompt},    \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract and process the answer\n",
    "    answer = json.loads(completion.choices[0].message.content) if isinstance(completion.choices[0].message.content, str) else completion.choices[0].message.content\n",
    "    winning_model_id = answer['winning_model_id']\n",
    "    report = answer['report']\n",
    "\n",
    "    # Update the 'report' column and reorder the columns\n",
    "    results['report'] = results['model_id'].apply(lambda x: report if x == winning_model_id else '')\n",
    "    cols_order = ['model_id', 'completion_model_id', 'splitter_id', 'embedding_model_id', 'prediction_similarity', 'accuracy', 'report']\n",
    "    res_df = results[cols_order]\n",
    "\n",
    "    # Save results to a csv file\n",
    "    res_df.to_csv('data/grid-search-results.csv', index=False)\n",
    "\n",
    "\n",
    "generate_report_and_save(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
